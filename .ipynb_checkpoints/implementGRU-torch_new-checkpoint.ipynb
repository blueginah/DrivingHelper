{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d03d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57751f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "# from optuna.trial import TrialState\n",
    "import os\n",
    "import joblib\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0feeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-20 21:18:34,183]\u001b[0m A new study created in memory with name: no-name-fb3d68ef-d8aa-4eb1-876d-826155981fde\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10] [19 20 21 22 23 24 25 26 27 28 29]\n",
      "[ 3  5  7  8  1  4 10  6  9  0  2 22 24 26 27 20 23 29 25 28 19 21 12 15\n",
      " 14 16 17 18 11 13 31 34 33 35 36 37 30 32]\n",
      "Epoch: [0/50]Loss: 0.8493\n",
      "Accuracy of the network on the 16 test images: 83.33333333333333 %\n",
      "Epoch: [1/50]Loss: 0.6984\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [2/50]Loss: 0.6154\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [3/50]Loss: 0.5719\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [4/50]Loss: 0.4955\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [5/50]Loss: 0.3755\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [6/50]Loss: 0.2250\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [7/50]Loss: 0.0929\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [8/50]Loss: 0.0357\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [9/50]Loss: 0.0108\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [10/50]Loss: 0.0071\n",
      "Accuracy of the network on the 16 test images: 66.66666666666667 %\n",
      "Epoch: [11/50]Loss: 0.0068\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [12/50]Loss: 0.0042\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [13/50]Loss: 0.0022\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [14/50]Loss: 0.0014\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [15/50]Loss: 0.0011\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [16/50]Loss: 0.0009\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [17/50]Loss: 0.0008\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [18/50]Loss: 0.0007\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [19/50]Loss: 0.0006\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [20/50]Loss: 0.0005\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [21/50]Loss: 0.0005\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [22/50]Loss: 0.0005\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [23/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [24/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [25/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [26/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [27/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [28/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [29/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [30/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [31/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [32/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [33/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [34/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [35/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [36/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [37/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [38/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [39/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [40/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [41/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [42/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [43/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [44/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [45/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [46/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [47/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [48/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [49/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-20 21:19:03,742]\u001b[0m Trial 0 finished with value: 91.66666666666667 and parameters: {'lr': 0.027666252897383857, 'hidden_size': 94}. Best is trial 0 with value: 91.66666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Loss: 0.0002\n",
      "Epoch: [0/50]Loss: 0.2506\n",
      "Accuracy of the network on the 16 test images: 33.333333333333336 %\n",
      "Epoch: [1/50]Loss: 0.4917\n",
      "Accuracy of the network on the 16 test images: 33.333333333333336 %\n",
      "Epoch: [2/50]Loss: 0.6384\n",
      "Accuracy of the network on the 16 test images: 83.33333333333333 %\n",
      "Epoch: [3/50]Loss: 0.5921\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [4/50]Loss: 0.5374\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [5/50]Loss: 0.4592\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [6/50]Loss: 0.3230\n",
      "Accuracy of the network on the 16 test images: 75.0 %\n",
      "Epoch: [7/50]Loss: 0.1761\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [8/50]Loss: 0.1187\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [9/50]Loss: 0.0427\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [10/50]Loss: 0.0050\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [11/50]Loss: 0.0016\n",
      "Accuracy of the network on the 16 test images: 83.33333333333333 %\n",
      "Epoch: [12/50]Loss: 0.0025\n",
      "Accuracy of the network on the 16 test images: 83.33333333333333 %\n",
      "Epoch: [13/50]Loss: 0.0032\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [14/50]Loss: 0.0015\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [15/50]Loss: 0.0009\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [16/50]Loss: 0.0007\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [17/50]Loss: 0.0006\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [18/50]Loss: 0.0005\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [19/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [20/50]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [21/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [22/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [23/50]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [24/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [25/50]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-49cdb5a012d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'cornerData/mnist_optuna{curve_num}.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-49cdb5a012d9>\u001b[0m in \u001b[0;36mtrain_mnist\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    251\u001b[0m                     \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;31m#         optimizer.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_list = [0,1,2,3,4,5,0]\n",
    "number_of_corner_list = [1,2,3,4,5,6,6]\n",
    "feature_list = [0, 1]\n",
    "truncate_number = 60\n",
    "for i in range(1):\n",
    "    feature = feature_list[i]\n",
    "    for i in range(1):\n",
    "        start = start_list[i]\n",
    "        number_of_corner = number_of_corner_list[i]\n",
    "        df_begin = []\n",
    "        num_row = []\n",
    "        num_row_corner = []\n",
    "        percent = []\n",
    "    #     start = 0s\n",
    "    #     number_of_corner = 1\n",
    "        f_1 = 'beginner_expert_processedData/beginner/beginner_'\n",
    "        f_3 = '.csv'\n",
    "        num_begin = 19\n",
    "#         curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1212.3, 1437.1]]\n",
    "        curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1048.7, 1110.5], [1212.3, 1437.1]]\n",
    "#         curveList = [[103.9, 156.6], [156.6, 209.3], [316.6, 358.1], [358.1, 399.6], \n",
    "#                      [425.3, 471.6], [471.6, 517.9],p[590.5, 673.7], [673.7, 756.9],\n",
    "#                      [1048.7, 1079.6], [1079.6, 1110.5], [1212.3, 1324.7], [1324.7, 1437.1]]\n",
    "\n",
    "        df_concat = pd.DataFrame()\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            for idx in range(1, num_begin+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =0\n",
    "                tmp['curve_number'] = curve_num\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "                num_row.append(np.size(tmpcorner,0)) \n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "                df_begin.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_begin[idx-1]])      \n",
    "            df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_begin'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_begin = []\n",
    "        df_exp = []\n",
    "        f_1 = 'beginner_expert_processedData/expert/expert_'\n",
    "        f_3 = '.csv'\n",
    "        num_exp = 19\n",
    "        \n",
    "        \n",
    "        df_concat = pd.DataFrame()\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            for idx in range(1, num_exp+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =1\n",
    "                tmp['curve_number'] = curve_num\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "                num_row.append(np.size(tmpcorner,0))\n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "                df_exp.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_exp[idx-1]])\n",
    "            df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_expert'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_exp = []\n",
    "            num_row_corner = np.array(num_row_corner)\n",
    "            per = 60  ##truncate to 60\n",
    "            percent.append(per)\n",
    "            num_row_corner = []\n",
    "        sequence_length = min(num_row)\n",
    "        if sequence_length < truncate_number:\n",
    "            sequnce_length = truncate_number + 5\n",
    "        mean_row = round(np.mean(num_row))\n",
    "        mean_row = min(num_row)\n",
    "\n",
    "        if feature == 0:\n",
    "            left_column = [\n",
    "                'curve_number',\n",
    "                'GPS Latitude','GPS Longitude',\n",
    "                'CG Distance',\n",
    "                'Damper Velocity (Calc) FL','Damper Velocity (Calc) FR','Damper Velocity (Calc) RL',\n",
    "            'Damper Velocity (Calc) RR','Corr Speed','Brake Pos',\n",
    "            'CG Accel Lateral','CG Accel Longitudinal','CG Accel Vertical','CG Height','Camber FL','Camber FR','Camber RL','Camber RR','Car Coord X',\n",
    "            'Car Coord Y','Car Coord Z','Car Pos Norm','Chassis Pitch Angle','Chassis Pitch Rate','Chassis Roll Angle','Chassis Roll Rate',\n",
    "            'Chassis Velocity X','Chassis Velocity Y','Chassis Velocity Z','Chassis Yaw Rate','Drive Train Speed','Engine RPM','Ground Speed',\n",
    "            'Ride Height FL','Ride Height FR','Ride Height RL','Ride Height RR','Road Temp','Self Align Torque FL','Self Align Torque FR',\n",
    "            'Self Align Torque RL','Self Align Torque RR','Steering Angle','Suspension Travel FL','Suspension Travel FR',\n",
    "            'Suspension Travel RL','Suspension Travel RR','Tire Load FL','Tire Load FR','Tire Load RL','Tire Load RR','Tire Loaded Radius FL',\n",
    "            'Tire Loaded Radius FR','Tire Loaded Radius RL','Tire Loaded Radius RR','Tire Pressure FL','Tire Pressure FR','Tire Pressure RL','Tire Pressure RR',\n",
    "            'Tire Rubber Grip FL','Tire Rubber Grip FR','Tire Rubber Grip RL','Tire Rubber Grip RR','Tire Slip Angle FL','Tire Slip Angle FR',\n",
    "            'Tire Slip Angle RL','Tire Slip Angle RR','Tire Slip Ratio FL','Tire Slip Ratio FR','Tire Slip Ratio RL','Tire Slip Ratio RR',\n",
    "            'Tire Temp Core FL','Tire Temp Core FR','Tire Temp Core RL','Tire Temp Core RR','Tire Temp Inner FL','Tire Temp Inner FR',\n",
    "            'Tire Temp Inner RL','Tire Temp Inner RR','Tire Temp Middle FL','Tire Temp Middle FR','Tire Temp Middle RL',\n",
    "            'Tire Temp Middle RR','Tire Temp Outer FL','Tire Temp Outer FR','Tire Temp Outer RL','Tire Temp Outer RR','Toe In FL',\n",
    "            'Toe In FR','Toe In RL','Toe In RR','Wheel Angular Speed FL','Wheel Angular Speed FR','Wheel Angular Speed RL','Wheel Angular Speed RR',\n",
    "            'Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration','level']\n",
    "\n",
    "        else:\n",
    "            left_column = ['curve_number','Brake Pos', 'Ground Speed', 'Steering Angle', 'Throttle Pos', 'Chassis Yaw Rate', 'Chassis Velocity X',\n",
    "                               'Chassis Velocity Y','Chassis Velocity Z','Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration',\n",
    "                               'CG Distance',\n",
    "                           'level']\n",
    "\n",
    "        #Hyper-parameters\n",
    "        num_epochs = 50\n",
    "        batches = 1\n",
    "        learning_rate = 0.001\n",
    "        input_size = len(left_column)-1 # left column except 'level'\n",
    "        output_size = 2 # Expert and Beginner\n",
    "        hidden_size = 100 # ?\n",
    "        num_layers = 2\n",
    "        num_begin_train = round(num_begin*0.60)*(number_of_corner-start)\n",
    "        num_exp_train = round(num_exp*0.60)*(number_of_corner-start)\n",
    "        num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "        num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "        aug = 1\n",
    "\n",
    "        \n",
    "        class GRU(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "                super(GRU, self).__init__()\n",
    "                self.num_layers = num_layers\n",
    "                self.hidden_size = hidden_size\n",
    "                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "            def forward(self, x):\n",
    "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                out, _ = self.gru(x, h0)\n",
    "                out = out[:, -1, :]\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "        \n",
    "\n",
    "        ## Data Processing\n",
    "        array_x = []\n",
    "        array_y = []\n",
    "        input_x = []\n",
    "        input_y = []\n",
    "        n_row = []\n",
    "\n",
    "        df_tmp_begin = pd.DataFrame() \n",
    "        df_tmp_exp = pd.DataFrame() \n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            df_tmp_begin = pd.concat([df_tmp_begin,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_begin.csv')])\n",
    "            df_tmp_exp   = pd.concat([df_tmp_exp,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_expert.csv')])    \n",
    "        df_curve1 = pd.concat([df_tmp_begin, df_tmp_exp], ignore_index=True) \n",
    "        df_curve1 = df_curve1.loc[:,left_column]\n",
    "        df_curve1_saved = df_curve1.loc[:,left_column] # data backup\n",
    "        df_curve1.to_csv('cornerData/corner_'+'_dfcurve1'+'.csv')\n",
    "        datum = df_curve1_saved\n",
    "        yyy = datum.pop('level')\n",
    "        left = left_column.remove('level')\n",
    "        for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "            y = yyy.loc[0:num_row[i]-1]\n",
    "            x_original = datum.loc[0:num_row[i]-1]\n",
    "            scaler = MinMaxScaler()\n",
    "            x_normal = scaler.fit_transform(x_original)\n",
    "            x_normal = scaler.transform(x_original)\n",
    "            x_normal = np.pad(x_normal,[(0,60),(0,0)],'edge') #post padding ## edge padding\n",
    "            x = pd.DataFrame(x_normal,columns=left)\n",
    "            p = i//(num_begin*2)\n",
    "            x = x.truncate(after=percent[p]-1)\n",
    "            datum.drop(range(0,num_row[i]),inplace=True)\n",
    "            datum.reset_index(drop=True, inplace=True)\n",
    "            yyy.drop(range(0,num_row[i]),inplace=True)\n",
    "            yyy.reset_index(drop=True, inplace=True)\n",
    "            array_x.append(x)\n",
    "            array_y.append(y)\n",
    "\n",
    "\n",
    "\n",
    "        ## Randomize sequence \n",
    "        # sequence = np.arange((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug/2)\n",
    "        seq_train_begin = np.arange(num_begin_train)\n",
    "        seq_test_begin = np.arange(num_begin_test) + num_begin_train\n",
    "        seq_train_exp = seq_train_begin + num_begin*(number_of_corner-start)\n",
    "        seq_test_exp = seq_test_begin + num_begin*(number_of_corner-start)\n",
    "        print(seq_train_begin,seq_train_exp)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_train_begin)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_test_begin)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_train_exp)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_test_exp)\n",
    "\n",
    "\n",
    "        sequence = np.concatenate((seq_train_begin, seq_train_exp, seq_test_begin, seq_test_exp), axis=None)\n",
    "        sequence = sequence.astype('int')\n",
    "        print(sequence)\n",
    "\n",
    "\n",
    "        for i in sequence:\n",
    "            input_x.append(array_x[i])\n",
    "            input_y.append(array_y[i])\n",
    "            p = i//(num_begin*2)\n",
    "            n_row = n_row + [percent[p]]\n",
    "        \n",
    "    \n",
    "            \n",
    "        def train_mnist(trial):\n",
    "            \n",
    "            d2 = {'test sample': [], 'loss': [], 'accuracy': [], 'epoch' : [], 'batch':[],'learning rate' :[], 'hidden size':[],'hidden_layer':[]}\n",
    "            df2 = pd.DataFrame(data=d2)\n",
    "            loss_list = []\n",
    "            iteration_list = []\n",
    "            accuracy_list = []\n",
    "            test_list=[]\n",
    "            accuracy2_list=[]\n",
    "            count = 0\n",
    "            cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "              'train_batch_size' : 64,\n",
    "              'test_batch_size' : 1000,\n",
    "              'n_epochs' : 1,\n",
    "              'seed' : 0,\n",
    "              'log_interval' : 100,\n",
    "              'save_model' : False,\n",
    "              'lr'       : 0.0008674236780492814,          \n",
    "              'hidden_size' : 126,\n",
    "              'lr'       : trial.suggest_loguniform('lr', 5e-3, 5e-2),          \n",
    "              'hidden_size' : trial.suggest_int('hidden_size', 60, 200),\n",
    "              'optimizer': optim.Adam,\n",
    "              'activation': F.relu}\n",
    "        \n",
    "            torch.manual_seed(cfg['seed'])\n",
    "        \n",
    "            gru = GRU(input_size, cfg['hidden_size'], num_layers, output_size)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)       \n",
    "#             optimizer = cfg['optimizer'](gru.parameters(), lr=cfg['lr'])\n",
    "    \n",
    "    \n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0,(num_begin_train + num_exp_train)*aug):\n",
    "                    # array type (numpy) ì•ž\n",
    "                    X = np.array(input_x[i])\n",
    "                    X = X.reshape(-1,n_row[i],input_size)\n",
    "                    Y = np.array(input_y[i])\n",
    "\n",
    "                    # tensor type (pytorch)\n",
    "                    X = torch.from_numpy(X)\n",
    "                    X = X.float()\n",
    "                    Y = torch.tensor([Y[0]])\n",
    "                    Y = Y.type(torch.LongTensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = gru(X)\n",
    "                    loss = criterion(output, Y)\n",
    "\n",
    "                    # Backward and optimize\n",
    "            #         optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                count += 1\n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                print (f'Epoch: [{epoch}/{num_epochs}]' f'Loss: {loss.item():.4f}')\n",
    "            ## Test\n",
    "                with torch.no_grad():\n",
    "                    n_correct = 0\n",
    "                    n_correct2 = 0\n",
    "                    n_samples = 0\n",
    "                    n_samples2 = 0\n",
    "\n",
    "                    for i in range((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug-4):\n",
    "\n",
    "                        # array type (numpy)\n",
    "                        X = np.array(input_x[i])\n",
    "                        X = X.reshape(-1,n_row[i],input_size)\n",
    "                        Y = np.array(input_y[i])\n",
    "\n",
    "                        # tensor type (pytorch)\n",
    "                        X = torch.from_numpy(X)\n",
    "                        X = X.float()\n",
    "                        Y = torch.tensor([Y[0]])\n",
    "                        Y = Y.type(torch.LongTensor)\n",
    "                        output = gru(X)\n",
    "                        _, predicted = torch.max(output.data, 1)\n",
    "                        n_samples += Y.size(0)\n",
    "                        n_correct += (predicted == Y).sum().item()\n",
    "\n",
    "    \n",
    "                    for i in range((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug-4,(num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug):\n",
    "\n",
    "                        # array type (numpy)\n",
    "                        X = np.array(input_x[i])\n",
    "                        X = X.reshape(-1,n_row[i],input_size)\n",
    "                        Y = np.array(input_y[i])\n",
    "\n",
    "                        # tensor type (pytorch)\n",
    "                        X = torch.from_numpy(X)\n",
    "                        X = X.float()\n",
    "                        Y = torch.tensor([Y[0]])\n",
    "                        Y = Y.type(torch.LongTensor)\n",
    "                        output = gru(X)\n",
    "                        _, predicted2 = torch.max(output.data, 1)\n",
    "                        n_samples2 += Y.size(0)\n",
    "                        n_correct2 += (predicted2 == Y).sum().item()\n",
    "    \n",
    "    \n",
    "                    acc = 100.0 * n_correct / n_samples\n",
    "                    acc2 = 100.0 * n_correct2 / n_samples2\n",
    "                    accuracy_list.append(acc)\n",
    "                    accuracy2_list.append(acc2)\n",
    "                    print(f'Accuracy of the network on the {(num_begin_test + num_exp_test)*aug} test images: {acc} %')          \n",
    "            plt.plot(iteration_list,loss_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"GRU: Loss vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1{curve_num}.png')\n",
    "                plt.clf()\n",
    "\n",
    "            plt.plot(iteration_list,accuracy_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\" Validation Accuracy\")\n",
    "            plt.title(\"GRU: Validaiton Accuracy vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1_acc{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1_acc{curve_num}.png')\n",
    "                plt.clf()    \n",
    "\n",
    "            plt.plot(iteration_list,accuracy2_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\" Test Accuracy\")\n",
    "            plt.title(\"GRU: Test Accuracy vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1_acc{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1_acc{curve_num}.png')\n",
    "                plt.clf()  \n",
    "\n",
    "            print(f'Accuracy of the network on the {num_begin_test + num_exp_test} test images: {acc} %')\n",
    "            print(f'Loss: {loss.item():.4f}')\n",
    "            return acc\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(train_mnist, n_trials=10)\n",
    "        joblib.dump(study, f'cornerData/mnist_optuna{curve_num}.pkl')\n",
    "    \n",
    "        study = joblib.load(f'cornerData/mnist_optuna{curve_num}.pkl')\n",
    "        df3 = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "        print(df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
