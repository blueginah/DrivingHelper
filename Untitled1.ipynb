{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2929f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13b39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5062ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 <class 'numpy.int32'>\n",
      "[72, 71, 78, 91, 68, 69, 75, 84, 76, 79, 97, 102, 101, 89, 63, 74, 80, 69, 78, 68, 64, 64, 71, 68, 65, 66, 66, 67, 66, 72, 66, 67, 70, 68, 62, 68, 66, 72]\n",
      "102\n",
      "[74]\n",
      "12 12 7 7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_noraml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-903701b7990f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mx_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_normal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnum_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'edge'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#post padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_noraml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;31m#     x_normal = np.pad(x_normal,[(sequence_length-num_row[i],0),(0,0)]) #pre padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_noraml' is not defined"
     ]
    }
   ],
   "source": [
    "start_list = [0,1,2,3,4,5,0]\n",
    "number_of_corner_list = [1,2,3,4,5,6,6]\n",
    "d2 = {'test sample': [], 'loss': [], 'accuracy': [], 'epoch' : [], 'batch':[],'learning rate' :[], 'hidden size':[],'hidden_layer':[]}\n",
    "df2 = pd.DataFrame(data=d2)\n",
    "feature_list = [0, 1]\n",
    "for i in range(2):\n",
    "    feature = feature_list[i]\n",
    "    for i in range(7):\n",
    "        start = start_list[i]\n",
    "        number_of_corner = number_of_corner_list[i]\n",
    "        df_begin = []\n",
    "        num_row = []\n",
    "        num_row_corner = []\n",
    "        percent = []\n",
    "    #     start = 0\n",
    "    #     number_of_corner = 1\n",
    "        f_1 = 'beginner_expert_processedData/beginner/beginner_'\n",
    "        f_3 = '.csv'\n",
    "        num_begin = 19\n",
    "        curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1048.7, 1110.5], [1212.3, 1437.1]]\n",
    "\n",
    "        df_concat = pd.DataFrame()\n",
    "\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "        # for curve_num in [0,3]:\n",
    "        #     print(num_row)\n",
    "            for idx in range(1, num_begin+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =0\n",
    "                tmp['curve_number'] = curve_num\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "\n",
    "\n",
    "                num_row.append(np.size(tmpcorner,0)) \n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "                df_begin.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_begin[idx-1]])      \n",
    "\n",
    "            df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_begin'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_begin = []\n",
    "\n",
    "\n",
    "          #######  #######  #######  #######  #######  #######  #######  #######  #######  \n",
    "        df_exp = []\n",
    "        f_1 = 'beginner_expert_processedData/expert/expert_'\n",
    "        f_3 = '.csv'\n",
    "        num_exp = 19\n",
    "\n",
    "        df_concat = pd.DataFrame()\n",
    "\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "        # for curve_num in [0,3]:\n",
    "            for idx in range(1, num_exp+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =1\n",
    "                tmp['curve_number'] = curve_num\n",
    "\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "                num_row.append(np.size(tmpcorner,0))\n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "\n",
    "                df_exp.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_exp[idx-1]])\n",
    "            df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_expert'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_exp = []\n",
    "            num_row_corner = np.array(num_row_corner)\n",
    "            per = np.percentile(num_row_corner, 70).astype('int')\n",
    "        #     np.ndarray.tolist(per)\n",
    "            print(per,type(per))\n",
    "            percent.append(per)\n",
    "            num_row_corner = []\n",
    "\n",
    "          #######  #######  #######  #######  #######  #######  #######  #######  #######  \n",
    "\n",
    "        print(num_row)\n",
    "        sequence_length = max(num_row)\n",
    "        print(sequence_length)\n",
    "        mean_row = round(np.mean(num_row))\n",
    "        mean_row = min(num_row)\n",
    "        print(percent)\n",
    "\n",
    "\n",
    "          #######  #######  #######  #######  #######  #######  #######  #######  #######  \n",
    "        if feature == 0:\n",
    "            left_column = [\n",
    "            #'Time',\n",
    "            #     'Distance',\n",
    "            #     'Session Time Left',\n",
    "            #     'Corr Dist','Corr Dist (Unstretched)',\n",
    "                'GPS Latitude','GPS Longitude',\n",
    "                'CG Distance',\n",
    "                'Damper Velocity (Calc) FL','Damper Velocity (Calc) FR','Damper Velocity (Calc) RL',\n",
    "            'Damper Velocity (Calc) RR','Corr Speed','Brake Pos',\n",
    "            'CG Accel Lateral','CG Accel Longitudinal','CG Accel Vertical','CG Height','Camber FL','Camber FR','Camber RL','Camber RR','Car Coord X',\n",
    "            'Car Coord Y','Car Coord Z','Car Pos Norm','Chassis Pitch Angle','Chassis Pitch Rate','Chassis Roll Angle','Chassis Roll Rate',\n",
    "            'Chassis Velocity X','Chassis Velocity Y','Chassis Velocity Z','Chassis Yaw Rate','Drive Train Speed','Engine RPM','Ground Speed',\n",
    "            'Ride Height FL','Ride Height FR','Ride Height RL','Ride Height RR','Road Temp','Self Align Torque FL','Self Align Torque FR',\n",
    "            'Self Align Torque RL','Self Align Torque RR','Steering Angle','Suspension Travel FL','Suspension Travel FR',\n",
    "            'Suspension Travel RL','Suspension Travel RR','Tire Load FL','Tire Load FR','Tire Load RL','Tire Load RR','Tire Loaded Radius FL',\n",
    "            'Tire Loaded Radius FR','Tire Loaded Radius RL','Tire Loaded Radius RR','Tire Pressure FL','Tire Pressure FR','Tire Pressure RL','Tire Pressure RR',\n",
    "            'Tire Rubber Grip FL','Tire Rubber Grip FR','Tire Rubber Grip RL','Tire Rubber Grip RR','Tire Slip Angle FL','Tire Slip Angle FR',\n",
    "            'Tire Slip Angle RL','Tire Slip Angle RR','Tire Slip Ratio FL','Tire Slip Ratio FR','Tire Slip Ratio RL','Tire Slip Ratio RR',\n",
    "            'Tire Temp Core FL','Tire Temp Core FR','Tire Temp Core RL','Tire Temp Core RR','Tire Temp Inner FL','Tire Temp Inner FR',\n",
    "            'Tire Temp Inner RL','Tire Temp Inner RR','Tire Temp Middle FL','Tire Temp Middle FR','Tire Temp Middle RL',\n",
    "            'Tire Temp Middle RR','Tire Temp Outer FL','Tire Temp Outer FR','Tire Temp Outer RL','Tire Temp Outer RR','Toe In FL',\n",
    "            'Toe In FR','Toe In RL','Toe In RR','Wheel Angular Speed FL','Wheel Angular Speed FR','Wheel Angular Speed RL','Wheel Angular Speed RR',\n",
    "            'Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration','level']\n",
    "\n",
    "        else:\n",
    "            left_column = ['Brake Pos', 'Ground Speed', 'Steering Angle', 'Throttle Pos', 'Chassis Yaw Rate', 'Chassis Velocity X',\n",
    "                               'Chassis Velocity Y','Chassis Velocity Z','Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration',\n",
    "                               'CG Distance',\n",
    "                           'level']\n",
    "\n",
    "        #Hyper-parameters\n",
    "        num_epochs = 120\n",
    "        batches = 1\n",
    "        learning_rate = 0.001\n",
    "        input_size = len(left_column)-1 # left column except 'level'\n",
    "        output_size = 2 # Expert and Beginner\n",
    "        hidden_size = 62 # ?\n",
    "        num_layers = 2\n",
    "        num_begin_train = round(num_begin*0.65)*(number_of_corner-start)\n",
    "        num_exp_train = round(num_exp*0.65)*(number_of_corner-start)\n",
    "        num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "        num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "\n",
    "        print(num_begin_train, num_exp_train,num_begin_test,num_exp_test)\n",
    "        aug = 1\n",
    "        ## Define GRU, Loss func and Optimizer\n",
    "        class GRU(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "                super(GRU, self).__init__()\n",
    "                self.num_layers = num_layers\n",
    "                self.hidden_size = hidden_size\n",
    "    #             self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "            def forward(self, x):\n",
    "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                out, _ = self.gru(x, h0)\n",
    "    #             out, _ = self.lstm(x, (h0,c0)) \n",
    "                out = out[:, -1, :]\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        gru = GRU(input_size, hidden_size, num_layers, output_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)  \n",
    "\n",
    "        # gru.fc.weight.data.fill_(1)\n",
    "        # gru.fc.bias.data.fill_(1)\n",
    "    #     print(gru.fc.weight,gru.fc.bias)\n",
    "\n",
    "        ## Data Processing\n",
    "        array_x = []\n",
    "        array_y = []\n",
    "        input_x = []\n",
    "        input_y = []\n",
    "        n_row = []\n",
    "\n",
    "        df_tmp_begin = pd.DataFrame() \n",
    "        df_tmp_exp = pd.DataFrame() \n",
    "        for curve_num in range(start,number_of_corner):\n",
    "        # for curve_num in [0,3]:\n",
    "            df_tmp_begin = pd.concat([df_tmp_begin,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_begin.csv')])\n",
    "            df_tmp_exp   = pd.concat([df_tmp_exp,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_expert.csv')])    \n",
    "        df_curve1 = pd.concat([df_tmp_begin, df_tmp_exp], ignore_index=True) \n",
    "        df_curve1 = df_curve1.loc[:,left_column]\n",
    "        df_curve1_saved = df_curve1.loc[:,left_column] # data backup\n",
    "        df_curve1.to_csv('cornerData/corner_'+'_dfcurve1'+'.csv')\n",
    "\n",
    "\n",
    "\n",
    "        datum = df_curve1_saved\n",
    "        yyy = datum.pop('level')\n",
    "        left = left_column.remove('level')\n",
    "        for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "        #     x = df_curve1_saved.loc[0:num_row[i]-1\n",
    "            y = yyy.loc[0:num_row[i]-1]\n",
    "        #     y = y.iloc[0]\n",
    "            x_original = datum.loc[0:num_row[i]-1]\n",
    "\n",
    "        #     print(x_original)\n",
    "        #     print(num_row[i],y[0],x_original.iloc[-1,0])\n",
    "\n",
    "        #     scaler = StandardScaler()\n",
    "            scaler = MinMaxScaler()\n",
    "        #     scaler.fit(x_original)\n",
    "        #     scaler.mean_\n",
    "            x_normal = scaler.fit_transform(x_original)\n",
    "            x_normal = scaler.transform(x_original)\n",
    "\n",
    "\n",
    "            x_normal = np.pad(x_normal,[(0,sequence_length-num_row[i]),(0,0)],'edge') #post padding\n",
    "            print(x_normal)\n",
    "        #     x_normal = np.pad(x_normal,[(sequence_length-num_row[i],0),(0,0)]) #pre padding\n",
    "\n",
    "\n",
    "            x = pd.DataFrame(x_normal,columns=left)\n",
    "            p = i//(num_begin*2)\n",
    "            x = x.truncate(after=percent[p]-1)\n",
    "        #     print(x.shape)\n",
    "        #     print(datum)\n",
    "        #     print(i)\n",
    "        #     print(num_row)\n",
    "        #     print(num_row[i])\n",
    "            datum.drop(range(0,num_row[i]),inplace=True)\n",
    "            datum.reset_index(drop=True, inplace=True)\n",
    "            yyy.drop(range(0,num_row[i]),inplace=True)\n",
    "            yyy.reset_index(drop=True, inplace=True)\n",
    "            # y = x.pop('level')\n",
    "\n",
    "        #     # DATA Augmentation\n",
    "        #     nan = pd.DataFrame(np.nan,columns=range(x.shape[1]),index=range(x.shape[0]))\n",
    "        #     alter = pd.concat([x,nan]).sort_index()\n",
    "        #     alter = alter.interpolate()\n",
    "        #     alter.reset_index(drop=True, inplace=True)\n",
    "        #     x_aug = alter[alter.index%2==1]\n",
    "\n",
    "\n",
    "            array_x.append(x)\n",
    "        #     array_x.append(x_aug)\n",
    "            array_y.append(y)\n",
    "        #     array_y.append(y)\n",
    "\n",
    "\n",
    "        ## Randomize sequence \n",
    "        # sequence = np.arange((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug/2)\n",
    "        seq_train_begin = np.arange(num_begin_train)\n",
    "        seq_test_begin = np.arange(num_begin_test) + num_begin_train\n",
    "        seq_train_exp = seq_train_begin + num_begin*(number_of_corner-start)\n",
    "        seq_test_exp = seq_test_begin + num_begin*(number_of_corner-start)\n",
    "        print(seq_train_begin,seq_train_exp)\n",
    "        np.random.seed(12)\n",
    "        np.random.shuffle(seq_train_begin)\n",
    "        np.random.seed(12)\n",
    "        np.random.shuffle(seq_test_begin)\n",
    "        np.random.seed(12)\n",
    "        np.random.shuffle(seq_train_exp)\n",
    "        np.random.seed(12)\n",
    "        np.random.shuffle(seq_test_exp)\n",
    "        # seq_train2 = seq_test\n",
    "\n",
    "\n",
    "        sequence = np.concatenate((seq_train_begin, seq_train_exp, seq_test_begin, seq_test_exp), axis=None)\n",
    "        sequence = sequence.astype('int')\n",
    "        # sequence = [0,1,2,15,4,5,6,7,8,9,18,11,12,13,14,19,20,21,34,23,24,25,26,27,28,37,30,31,32,33,3,16,17,10,22,35,36,29]\n",
    "        print(sequence)\n",
    "\n",
    "        # # Data Augmentation\n",
    "        # num_row = pd.Series(num_row)\n",
    "        # num_row = num_row.repeat(2)\n",
    "        # # sequence = pd.Series(sequence)\n",
    "        # # sequence = sequence.repeat(2)\n",
    "        # num_row.reset_index(drop=True, inplace=True)\n",
    "        # # sequence.reset_index(drop=True, inplace=True)\n",
    "        # print(num_row, sequence)\n",
    "\n",
    "        # for i in range(len(percent)):\n",
    "        #     n_row = n_row + [percent[i]]*num_begin*2\n",
    "        for i in sequence:\n",
    "            input_x.append(array_x[i])\n",
    "            input_y.append(array_y[i])\n",
    "            p = i//(num_begin*2)\n",
    "        #     print(p)\n",
    "            n_row = n_row + [percent[p]]\n",
    "\n",
    "\n",
    "        #     n_row.append(num_row[i])\n",
    "        #     n_row.append(sequence_length)\n",
    "        #     n_row.append(mean_row+1-20)\n",
    "        # input_x = np.array(input_x)\n",
    "        # input_y = np.array(input_y)\n",
    "        print(n_row)\n",
    "\n",
    "        ## Train \n",
    "        loss_list = []\n",
    "        iteration_list = []\n",
    "        accuracy_list = []\n",
    "        test_list=[]\n",
    "        count = 0\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "        print((num_begin_train + num_exp_train)*aug)\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(0,(num_begin_train + num_exp_train)*aug):\n",
    "\n",
    "        #         print(i)\n",
    "        #         print(len(input_x))\n",
    "                # array type (numpy) ì•ž\n",
    "                X = np.array(input_x[i])\n",
    "        #         print(X.shape)\n",
    "        #         X = input_x[i]\n",
    "                X = X.reshape(-1,n_row[i],input_size)\n",
    "\n",
    "                Y = np.array(input_y[i])\n",
    "\n",
    "        #         Y = input_y[i]\n",
    "        #         print(X,X.shape,Y[0])\n",
    "        #         time.sleep(300)\n",
    "                # tensor type (pytorch)\n",
    "                X = torch.from_numpy(X)\n",
    "                X = X.float()\n",
    "                Y = torch.tensor([Y[0]])\n",
    "        #         Y = torch.tensor([Y])\n",
    "                Y = Y.type(torch.LongTensor)\n",
    "        #         Y = Y.float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = gru(X)\n",
    "                loss = criterion(output, Y)\n",
    "\n",
    "                # Backward and optimize\n",
    "        #         optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            count += 1\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            print (f'Epoch: [{epoch}/{num_epochs}]' f'Loss: {loss.item():.4f}')\n",
    "        ## Test\n",
    "        with torch.no_grad():\n",
    "            n_correct = 0\n",
    "            n_samples = 0\n",
    "\n",
    "            for i in range((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug):\n",
    "\n",
    "                # array type (numpy)\n",
    "                X = np.array(input_x[i])\n",
    "        #         X = input_x[i]\n",
    "                X = X.reshape(-1,n_row[i],input_size)\n",
    "                Y = np.array(input_y[i])\n",
    "        #         Y = input_y[i]\n",
    "\n",
    "                # tensor type (pytorch)\n",
    "                X = torch.from_numpy(X)\n",
    "                X = X.float()\n",
    "                Y = torch.tensor([Y[0]])\n",
    "        #         Y = torch.tensor(Y)\n",
    "                Y = Y.type(torch.LongTensor)\n",
    "        #         Y = Y.float()\n",
    "                output = gru(X)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                n_samples += Y.size(0)\n",
    "                n_correct += (predicted == Y).sum().item()\n",
    "                print(Y, predicted)\n",
    "\n",
    "\n",
    "\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "        #     print(f'Accuracy of the network on the {(num_begin_test + num_exp_test)*aug} test images: {acc} %')\n",
    "\n",
    "        plt.plot(iteration_list,loss_list)\n",
    "        plt.xlabel(\"Number of iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "    #     plt.show()\n",
    "        if feature == 0:\n",
    "            plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat.png')\n",
    "        else:\n",
    "            plt.savefig(f'cornerData/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat.png')\n",
    "        print(f'Accuracy of the network on the {num_begin_test + num_exp_test} test images: {acc} %')\n",
    "        print(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "        d = {'test sample': [num_begin_test + num_exp_test], 'loss': [loss.item()], 'accuracy': [acc], 'epoch' : [num_epochs], 'batch':[batches],'learning rate' :[learning_rate], 'hidden size':[hidden_size],'hidden_layer':[num_layers]}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df2 = df2.append(df)\n",
    "\n",
    "    if feature == 0:\n",
    "        df2.to_csv('cornerData/result_gru_allFeature.csv')\n",
    "    else:\n",
    "        df2.to_csv('cornerData/result_gru_selectedFeature.csv')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21359608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2.to_csv('cornerData/result_gru.csv')\n",
    "# d2 = {'test sample': [], 'loss': [], 'accuracy': [], 'epoch' : [], 'batch':[],'learning rate' :[], 'hidden size':[],'hidden_layer':[]}\n",
    "# df2 = pd.DataFrame(data=d2)\n",
    "d = {'test sample': [num_begin_test + num_exp_test], 'loss': [loss.item()], 'accuracy': [acc], 'epoch' : [num_epochs], 'batch':[batches],'learning rate' :[learning_rate], 'hidden size':[hidden_size],'hidden_layer':[num_layers]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df2 = df2.append(df+4)\n",
    "print(df2)\n",
    "\n",
    "prit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = df_curve1_saved\n",
    "# yyy = datum.pop('level')\n",
    "left = left_column.remove('level')\n",
    "for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "#     x = df_curve1_saved.loc[0:num_row[i]-1\n",
    "    y = yyy.loc[0:num_row[i]-1]\n",
    "#     y = y.iloc[0]\n",
    "    x_original = datum.loc[0:num_row[i]-1]\n",
    "\n",
    "#     print(x_original)\n",
    "#     print(num_row[i],y[0],x_original.iloc[-1,0])\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "#     scaler.fit(x_original)\n",
    "#     scaler.mean_\n",
    "    x_normal = scaler.fit_transform(x_original)\n",
    "    x_normal = scaler.transform(x_original)\n",
    "\n",
    "\n",
    "    x_normal = np.pad(x_normal,[(0,sequence_length-num_row[i]),(0,0),'edge']) #post padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7ac405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.36071769 ... 0.05504587 0.78350515 0.6097561 ]\n",
      " [0.96138871 0.01091018 0.39748188 ... 0.06422018 0.39175258 0.31707317]\n",
      " [0.92764439 0.02062472 0.36068906 ... 0.         0.51546392 0.41463414]\n",
      " ...\n",
      " [0.05759247 1.         0.17764531 ... 0.68807339 0.9072165  0.14634146]\n",
      " [0.05759247 1.         0.17764531 ... 0.68807339 0.9072165  0.14634146]\n",
      " [0.05759247 1.         0.17764531 ... 0.68807339 0.9072165  0.14634146]]\n"
     ]
    }
   ],
   "source": [
    "x_normal = np.pad(x_normal,[(0,sequence_length-num_row[i]),(0,0)],'edge') #post padding\n",
    "print(x_normal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
