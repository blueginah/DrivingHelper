{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make each level to each Curve CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_begin = []\n",
    "num_row = []\n",
    "start = 3\n",
    "number_of_corner = 6\n",
    "f_1 = 'beginner_expert_processedData/beginner/beginner_'\n",
    "f_3 = '.csv'\n",
    "num_begin = 25\n",
    "curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1048.7, 1110.5], [1212.3, 1437.1]]\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for curve_num in range(start,number_of_corner):\n",
    "#     print(num_row)\n",
    "    for idx in range(1, num_begin+1):\n",
    "        tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "        df = pd.read_csv(tmp_file)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        tmp = df.astype(float)\n",
    "        tmp['level'] =0\n",
    "        \n",
    "        tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "        num_row.append(np.size(tmpcorner,0)) \n",
    "        \n",
    "        df_begin.append(tmpcorner)\n",
    "        df_concat = pd.concat([df_concat,df_begin[idx-1]])      \n",
    "           \n",
    "    df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_begin'+'.csv')\n",
    "    df_concat = pd.DataFrame()\n",
    "    df_begin = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = []\n",
    "f_1 = 'beginner_expert_processedData/expert/expert_'\n",
    "f_3 = '.csv'\n",
    "num_exp = 19\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for curve_num in range(start,number_of_corner):\n",
    "    for idx in range(1, num_exp+1):\n",
    "        tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "        df = pd.read_csv(tmp_file)\n",
    "        df = df.dropna()\n",
    "\n",
    "        tmp = df.astype(float)\n",
    "        tmp['level'] =1\n",
    "\n",
    "        tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "        num_row.append(np.size(tmpcorner,0)) \n",
    "\n",
    "        df_exp.append(tmpcorner)\n",
    "        df_concat = pd.concat([df_concat,df_exp[idx-1]])\n",
    "    df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_expert'+'.csv')\n",
    "    df_concat = pd.DataFrame()\n",
    "    df_exp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 98, 105, 88, 115, 105, 112, 95, 94, 108, 127, 140, 135, 99, 97, 86, 90, 111, 97, 78, 79, 95, 82, 79, 84, 28, 27, 26, 27, 31, 31, 30, 30, 34, 30, 41, 36, 37, 33, 36, 29, 28, 31, 25, 27, 27, 28, 27, 25, 26, 125, 148, 151, 146, 129, 147, 145, 131, 145, 146, 154, 150, 194, 156, 183, 136, 142, 121, 494, 139, 141, 130, 148, 149, 133, 87, 86, 86, 84, 81, 81, 111, 88, 80, 78, 77, 77, 84, 80, 77, 79, 79, 80, 84, 25, 26, 26, 26, 27, 25, 25, 26, 26, 25, 25, 25, 25, 26, 25, 25, 25, 25, 25, 123, 132, 124, 125, 117, 126, 129, 119, 126, 122, 133, 126, 128, 125, 138, 117, 127, 133, 131]\n"
     ]
    }
   ],
   "source": [
    "print(num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# import time\n",
    "\n",
    "left_column = [\n",
    "#'Time',\n",
    "    'Distance','GPS Latitude','GPS Longitude','Damper Velocity (Calc) FL','Damper Velocity (Calc) FR','Damper Velocity (Calc) RL',\n",
    "'Damper Velocity (Calc) RR','Corr Dist','Corr Dist (Unstretched)','Corr Speed','Brake Pos',\n",
    "'CG Accel Lateral','CG Accel Longitudinal','CG Accel Vertical','CG Height','Camber FL','Camber FR','Camber RL','Camber RR','Car Coord X',\n",
    "'Car Coord Y','Car Coord Z','Car Pos Norm','Chassis Pitch Angle','Chassis Pitch Rate','Chassis Roll Angle','Chassis Roll Rate',\n",
    "'Chassis Velocity X','Chassis Velocity Y','Chassis Velocity Z','Chassis Yaw Rate','Drive Train Speed','Engine RPM','Ground Speed',\n",
    "'Ride Height FL','Ride Height FR','Ride Height RL','Ride Height RR','Road Temp','Self Align Torque FL','Self Align Torque FR',\n",
    "'Self Align Torque RL','Self Align Torque RR','Session Time Left','Steering Angle','Suspension Travel FL','Suspension Travel FR',\n",
    "'Suspension Travel RL','Suspension Travel RR','Tire Load FL','Tire Load FR','Tire Load RL','Tire Load RR','Tire Loaded Radius FL',\n",
    "'Tire Loaded Radius FR','Tire Loaded Radius RL','Tire Loaded Radius RR','Tire Pressure FL','Tire Pressure FR','Tire Pressure RL','Tire Pressure RR',\n",
    "'Tire Rubber Grip FL','Tire Rubber Grip FR','Tire Rubber Grip RL','Tire Rubber Grip RR','Tire Slip Angle FL','Tire Slip Angle FR',\n",
    "'Tire Slip Angle RL','Tire Slip Angle RR','Tire Slip Ratio FL','Tire Slip Ratio FR','Tire Slip Ratio RL','Tire Slip Ratio RR',\n",
    "'Tire Temp Core FL','Tire Temp Core FR','Tire Temp Core RL','Tire Temp Core RR','Tire Temp Inner FL','Tire Temp Inner FR',\n",
    "'Tire Temp Inner RL','Tire Temp Inner RR','Tire Temp Middle FL','Tire Temp Middle FR','Tire Temp Middle RL',\n",
    "'Tire Temp Middle RR','Tire Temp Outer FL','Tire Temp Outer FR','Tire Temp Outer RL','Tire Temp Outer RR','Toe In FL',\n",
    "'Toe In FR','Toe In RL','Toe In RR','Wheel Angular Speed FL','Wheel Angular Speed FR','Wheel Angular Speed RL','Wheel Angular Speed RR',\n",
    "'CG Distance','Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration','level']\n",
    "\n",
    "\n",
    "#Hyper-parameters\n",
    "num_epochs = 30\n",
    "batches = 10\n",
    "learning_rate = 0.01\n",
    "input_size = len(left_column)-1 # left column except 'level'\n",
    "output_size = 2 # Expert and Beginner\n",
    "hidden_size = 10 # ?\n",
    "num_layers = 2\n",
    "num_begin_train = round(num_begin*0.75)*(number_of_corner-start)\n",
    "num_exp_train = round(num_exp*0.75)*(number_of_corner-start)\n",
    "num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "\n",
    "print(num_begin_train, num_exp_train,num_begin_test,num_exp_test)\n",
    "aug = 1\n",
    "## Define GRU, Loss func and Optimizer\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "#         out, _ = self.lstm(x, (h0,c0)) \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "gru = GRU(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)  \n",
    "\n",
    "gru.fc.weight.data.fill_(1)\n",
    "gru.fc.bias.data.fill_(1)\n",
    "print(gru.fc.weight,gru.fc.bias)\n",
    "\n",
    "## Data Processing\n",
    "array_x = []\n",
    "array_y = []\n",
    "input_x = []\n",
    "input_y = []\n",
    "n_row = []\n",
    "\n",
    "df_tmp_begin = pd.DataFrame() \n",
    "df_tmp_exp = pd.DataFrame() \n",
    "for curve_num in range(start,number_of_corner):\n",
    "    df_tmp_begin = pd.concat([df_tmp_begin,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_begin.csv')])\n",
    "    df_tmp_exp   = pd.concat([df_tmp_exp,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_expert.csv')])    \n",
    "df_curve1 = pd.concat([df_tmp_begin, df_tmp_exp], ignore_index=True) \n",
    "df_curve1 = df_curve1.loc[:,left_column]\n",
    "df_curve1_saved = df_curve1.loc[:,left_column] # data backup\n",
    "df_curve1.to_csv('cornerData/corner_'+'_dfcurve1'+'.csv')\n",
    "\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x)\n",
    "# scaler.mean_\n",
    "# x = scaler.transform(x)\n",
    "# print(x_normal)\n",
    "# # print(x)\n",
    "# # print(x['Distance'])\n",
    "# print(type(x))\n",
    "# print(type(x_normal))\n",
    "\n",
    "datum = df_curve1_saved\n",
    "yyy = datum.pop('level')\n",
    "left = left_column.remove('level')\n",
    "for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "#     x = df_curve1_saved.loc[0:num_row[i]-1\n",
    "    y = yyy.loc[0:num_row[i]-1]\n",
    "    x_original = datum.loc[0:num_row[i]-1]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_original)\n",
    "    scaler.mean_\n",
    "    x_normal = scaler.transform(x_original)\n",
    "    x = pd.DataFrame(x_normal,columns=left)\n",
    "#     print(datum)\n",
    "#     print(i)\n",
    "#     print(num_row)\n",
    "#     print(num_row[i])\n",
    "    datum.drop(range(0,num_row[i]),inplace=True)\n",
    "    datum.reset_index(drop=True, inplace=True)\n",
    "    yyy.drop(range(0,num_row[i]),inplace=True)\n",
    "    yyy.reset_index(drop=True, inplace=True)\n",
    "    # y = x.pop('level')\n",
    "    \n",
    "#     # DATA Augmentation\n",
    "#     nan = pd.DataFrame(np.nan,columns=range(x.shape[1]),index=range(x.shape[0]))\n",
    "#     alter = pd.concat([x,nan]).sort_index()\n",
    "#     alter = alter.interpolate()\n",
    "#     alter.reset_index(drop=True, inplace=True)\n",
    "#     x_aug = alter[alter.index%2==1]\n",
    "\n",
    "    \n",
    "    array_x.append(x)\n",
    "#     array_x.append(x_aug)\n",
    "    array_y.append(y)\n",
    "#     array_y.append(y)\n",
    "\n",
    "    \n",
    "## Randomize sequence \n",
    "sequence = np.arange((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug)\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(sequence)\n",
    "\n",
    "# sequence = [0,1,2,15,4,5,6,7,8,9,18,11,12,13,14,19,20,21,34,23,24,25,26,27,28,37,30,31,32,33,3,16,17,10,22,35,36,29]\n",
    "print(sequence)\n",
    "\n",
    "# # Data Augmentation\n",
    "# num_row = pd.Series(num_row)\n",
    "# num_row = num_row.repeat(2)\n",
    "# # sequence = pd.Series(sequence)\n",
    "# # sequence = sequence.repeat(2)\n",
    "# num_row.reset_index(drop=True, inplace=True)\n",
    "# # sequence.reset_index(drop=True, inplace=True)\n",
    "# print(num_row, sequence)\n",
    "\n",
    "for i in sequence:\n",
    "    input_x.append(array_x[i])\n",
    "    input_y.append(array_y[i])\n",
    "    n_row.append(num_row[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "test_list=[]\n",
    "count = 0\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "print((num_begin_train + num_exp_train)*aug)\n",
    "\n",
    "# start = time.time()  # 시작 시간 저장\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0,(num_begin_train + num_exp_train)*aug):\n",
    "        \n",
    "#         print(i)\n",
    "#         print(len(input_x))\n",
    "        # array type (numpy) 앞\n",
    "        X = np.array(input_x[i])\n",
    "        X = X.reshape(-1,n_row[i],input_size)\n",
    "        Y = np.array(input_y[i])   \n",
    "        print(x[0,0],x.shape)\n",
    "        time.sleep(300)\n",
    "        # tensor type (pytorch)\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.float()\n",
    "        Y = torch.tensor([Y[0]])\n",
    "        Y = Y.type(torch.LongTensor)\n",
    "#         Y = Y.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = gru(X)\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         count += 1\n",
    "#         loss_list.append(loss.data)\n",
    "#         iteration_list.append(count)\n",
    "#         accuracy_list.append(accuracy)\n",
    "#         print (f'Loss: {loss.item():.4f}')\n",
    "    count += 1\n",
    "    loss_list.append(loss.data)\n",
    "    iteration_list.append(count)\n",
    "    print (f'Epoch: [{epoch}/{num_epochs}]' f'Loss: {loss.item():.4f}')\n",
    "## Test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for i in range((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug):\n",
    "        \n",
    "        # array type (numpy)\n",
    "        print(i)\n",
    "        X = np.array(input_x[i])\n",
    "        X = X.reshape(-1,n_row[i],input_size)\n",
    "        Y = np.array(input_y[i])   \n",
    "\n",
    "        # tensor type (pytorch)\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.float()\n",
    "        Y = torch.tensor([Y[0]])\n",
    "        Y = Y.type(torch.LongTensor)\n",
    "#         Y = Y.float()\n",
    "        output = gru(X)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_samples += Y.size(0)\n",
    "        n_correct += (predicted == Y).sum().item()\n",
    "        print(Y, predicted)\n",
    "        \n",
    "\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {(num_begin_test + num_exp_test)*aug} test images: {acc} %')\n",
    "#     print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "print(f'Accuracy of the network on the {num_begin_test + num_exp_test} test images: {acc} %')\n",
    "# print(predicted, Y, output)\n",
    "print(gru.fc.weight,gru.fc.bias)\n",
    "# print(iteration_list)\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
    "# plt.savefig('graph.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = 'bread'\n",
    "vars()['cat'] = 123\n",
    "print(cat)\n",
    "print(type(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame() \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([0, 1, np.nan, 10])\n",
    "s.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x)\n",
    "x.shape\n",
    "nan = pd.DataFrame(np.nan,columns=range(x.shape[1]),index=range(x.shape[0]))\n",
    "alter = pd.concat([x,nan]).sort_index()\n",
    "alter = alter.interpolate()\n",
    "alter.reset_index(drop=True, inplace=True)\n",
    "# print(alter[alter.index%2==0])\n",
    "# print(alter[alter.index%2==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# print(x['Distance'])\n",
    "# scaler = StandardScaler()\n",
    "# print(scaler.fit(x))\n",
    "# print(scaler.mean_)\n",
    "# # print(scaler.transform(x))\n",
    "# x_normal = scaler.transform(x)\n",
    "# print(x_normal)\n",
    "# # print(x)\n",
    "# # print(x['Distance'])\n",
    "# print(type(x))\n",
    "# print(type(x_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f\n",
    "print(gru.fc.weight,gru.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "a = np.array([1,2,3])\n",
    "a = pd.DataFrame([1,2,3])\n",
    "print(a)\n",
    "# a.repeat(2)\n",
    "# np.array(num_row).repeat(2)\n",
    "# num_row = pd.Series(num_row)\n",
    "# num_row.repeat(2)\n",
    "print(num_row)\n",
    "type(num_row)\n",
    "abc = pd.Series(num_row)\n",
    "abc.repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_begin_train = round(num_begin*0.75)*(number_of_corner-start)\n",
    "num_exp_train = round(num_exp*0.75)*(number_of_corner-start)\n",
    "num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "# print(num_begin_train, num_exp_train,num_begin_test,num_exp_test)\n",
    "num_begin_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
