{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make each level to each Curve CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_begin = []\n",
    "num_row = []\n",
    "start = 0\n",
    "number_of_corner = 6\n",
    "f_1 = 'beginner_expert_processedData/beginner/beginner_'\n",
    "f_3 = '.csv'\n",
    "num_begin = 19\n",
    "curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1048.7, 1110.5], [1212.3, 1437.1]]\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for curve_num in range(start,number_of_corner):\n",
    "#     print(num_row)\n",
    "    for idx in range(1, num_begin+1):\n",
    "        tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "        df = pd.read_csv(tmp_file)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        tmp = df.astype(float)\n",
    "        tmp['level'] =0\n",
    "        \n",
    "        tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "        num_row.append(np.size(tmpcorner,0)) \n",
    "        \n",
    "        df_begin.append(tmpcorner)\n",
    "        df_concat = pd.concat([df_concat,df_begin[idx-1]])      \n",
    "           \n",
    "    df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_begin'+'.csv')\n",
    "    df_concat = pd.DataFrame()\n",
    "    df_begin = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = []\n",
    "f_1 = 'beginner_expert_processedData/expert/expert_'\n",
    "f_3 = '.csv'\n",
    "num_exp = 19\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for curve_num in range(start,number_of_corner):\n",
    "    for idx in range(1, num_exp+1):\n",
    "        tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "        df = pd.read_csv(tmp_file)\n",
    "        df = df.dropna()\n",
    "\n",
    "        tmp = df.astype(float)\n",
    "        tmp['level'] =1\n",
    "\n",
    "        tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "        num_row.append(np.size(tmpcorner,0)) \n",
    "\n",
    "        df_exp.append(tmpcorner)\n",
    "        df_concat = pd.concat([df_concat,df_exp[idx-1]])\n",
    "    df_concat.to_csv('cornerData/corner_'+str(curve_num+1)+'_expert'+'.csv')\n",
    "    df_concat = pd.DataFrame()\n",
    "    df_exp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 71, 78, 91, 68, 69, 75, 84, 76, 79, 97, 102, 101, 89, 63, 74, 80, 69, 78, 46, 57, 49, 43, 50, 54, 61, 58, 50, 51, 59, 68, 50, 48, 69, 59, 60, 48, 52, 492, 214, 318, 311, 429, 102, 73, 77, 124, 94, 121, 218, 833, 695, 110, 90, 80, 329, 73, 111, 98, 105, 88, 115, 105, 112, 95, 94, 108, 127, 140, 135, 99, 97, 86, 90, 111, 97, 28, 27, 26, 27, 31, 31, 30, 30, 34, 30, 41, 36, 37, 33, 36, 29, 28, 31, 25, 125, 148, 151, 146, 129, 147, 145, 131, 145, 146, 154, 150, 194, 156, 183, 136, 142, 121, 494, 68, 64, 64, 71, 68, 65, 66, 66, 67, 66, 72, 66, 67, 70, 68, 62, 68, 66, 72, 51, 45, 46, 50, 53, 48, 47, 46, 44, 45, 52, 47, 45, 46, 47, 47, 43, 44, 43, 71, 68, 69, 67, 68, 81, 69, 69, 72, 77, 69, 70, 79, 71, 78, 69, 113, 77, 76, 87, 86, 86, 84, 81, 81, 111, 88, 80, 78, 77, 77, 84, 80, 77, 79, 79, 80, 84, 25, 26, 26, 26, 27, 25, 25, 26, 26, 25, 25, 25, 25, 26, 25, 25, 25, 25, 25, 123, 132, 124, 125, 117, 126, 129, 119, 126, 122, 133, 126, 128, 125, 138, 117, 127, 133, 131]\n",
      "833\n"
     ]
    }
   ],
   "source": [
    "print(num_row)\n",
    "sequence_length = max(num_row)\n",
    "print(sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 84 30 30\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "       requires_grad=True) Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "[122 187 101 194  26 156  86 188 190 219 189  55  62   3  36  22 213  54\n",
      " 154 207 130 116   9 135 134  77  88 180  53 211  11  60  99 226 120 112\n",
      " 127  33 126 102  41   2  14  16  52 157   0  94  61  21  98 174 172  83\n",
      " 121 128 178  20  95 203  29  25  63 158  46 199  69  30 175 144  56 148\n",
      " 123  59 151  42  57 220 103 221 181  73 165 182 108 216  32  15 195 169\n",
      " 146 214 149  66 179  35  51  97 222 115 163 215  28 166 177 143   4  12\n",
      " 162 170 225  49  64  10  87  85 137 147  93 202 204   5 200 159  39 173\n",
      "  58 113 104 201   6 205 111 106 139 164  65 209 100 141  31 118 119 208\n",
      "  38 150 217  50  84 167 197  75 124 105 206   8  40 110  47 186  48  18\n",
      "  43  17  90 117 227  19  78 212 196  72  68 136 131   1 218 133 184 168\n",
      " 193   7 142  44  27 185 138 107  89 198 155  23  96  79 210 192 114 171\n",
      "  70  67 145 152 224 223  37  74 129 140  34 132 125  45 160 176  92 109\n",
      "  24  82  71 161  76  13 183  81  91  80 191 153]\n",
      "168\n",
      "Epoch: [0/100]Loss: 0.5892\n",
      "Epoch: [1/100]Loss: 0.5536\n",
      "Epoch: [2/100]Loss: 0.7005\n",
      "Epoch: [3/100]Loss: 0.6973\n",
      "Epoch: [4/100]Loss: 0.6958\n",
      "Epoch: [5/100]Loss: 0.6936\n",
      "Epoch: [6/100]Loss: 0.6944\n",
      "Epoch: [7/100]Loss: 0.6940\n",
      "Epoch: [8/100]Loss: 0.6929\n",
      "Epoch: [9/100]Loss: 0.6930\n",
      "Epoch: [10/100]Loss: 0.6925\n",
      "Epoch: [11/100]Loss: 0.6925\n",
      "Epoch: [12/100]Loss: 0.6922\n",
      "Epoch: [13/100]Loss: 0.6923\n",
      "Epoch: [14/100]Loss: 0.6921\n",
      "Epoch: [15/100]Loss: 0.6921\n",
      "Epoch: [16/100]Loss: 0.6920\n",
      "Epoch: [17/100]Loss: 0.6920\n",
      "Epoch: [18/100]Loss: 0.6920\n",
      "Epoch: [19/100]Loss: 0.6921\n",
      "Epoch: [20/100]Loss: 0.6919\n",
      "Epoch: [21/100]Loss: 0.6921\n",
      "Epoch: [22/100]Loss: 0.6921\n",
      "Epoch: [23/100]Loss: 0.6921\n",
      "Epoch: [24/100]Loss: 0.6920\n",
      "Epoch: [25/100]Loss: 0.6917\n",
      "Epoch: [26/100]Loss: 0.6920\n",
      "Epoch: [27/100]Loss: 0.6921\n",
      "Epoch: [28/100]Loss: 0.6921\n",
      "Epoch: [29/100]Loss: 0.6921\n",
      "Epoch: [30/100]Loss: 0.6921\n",
      "Epoch: [31/100]Loss: 0.6922\n",
      "Epoch: [32/100]Loss: 0.6922\n",
      "Epoch: [33/100]Loss: 0.6922\n",
      "Epoch: [34/100]Loss: 0.6922\n",
      "Epoch: [35/100]Loss: 0.6922\n",
      "Epoch: [36/100]Loss: 0.6922\n",
      "Epoch: [37/100]Loss: 0.6922\n",
      "Epoch: [38/100]Loss: 0.6922\n",
      "Epoch: [39/100]Loss: 0.6922\n",
      "Epoch: [40/100]Loss: 0.6922\n",
      "Epoch: [41/100]Loss: 0.6922\n",
      "Epoch: [42/100]Loss: 0.6922\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import time\n",
    "\n",
    "# import time\n",
    "\n",
    "left_column = [\n",
    "#'Time',\n",
    "    'GPS Latitude','GPS Longitude','Damper Velocity (Calc) FL','Damper Velocity (Calc) FR','Damper Velocity (Calc) RL',\n",
    "'Damper Velocity (Calc) RR','Corr Dist','Corr Dist (Unstretched)','Corr Speed','Brake Pos',\n",
    "'CG Accel Lateral','CG Accel Longitudinal','CG Accel Vertical','CG Height','Camber FL','Camber FR','Camber RL','Camber RR','Car Coord X',\n",
    "'Car Coord Y','Car Coord Z','Car Pos Norm','Chassis Pitch Angle','Chassis Pitch Rate','Chassis Roll Angle','Chassis Roll Rate',\n",
    "'Chassis Velocity X','Chassis Velocity Y','Chassis Velocity Z','Chassis Yaw Rate','Drive Train Speed','Engine RPM','Ground Speed',\n",
    "'Ride Height FL','Ride Height FR','Ride Height RL','Ride Height RR','Road Temp','Self Align Torque FL','Self Align Torque FR',\n",
    "'Self Align Torque RL','Self Align Torque RR','Session Time Left','Steering Angle','Suspension Travel FL','Suspension Travel FR',\n",
    "'Suspension Travel RL','Suspension Travel RR','Tire Load FL','Tire Load FR','Tire Load RL','Tire Load RR','Tire Loaded Radius FL',\n",
    "'Tire Loaded Radius FR','Tire Loaded Radius RL','Tire Loaded Radius RR','Tire Pressure FL','Tire Pressure FR','Tire Pressure RL','Tire Pressure RR',\n",
    "'Tire Rubber Grip FL','Tire Rubber Grip FR','Tire Rubber Grip RL','Tire Rubber Grip RR','Tire Slip Angle FL','Tire Slip Angle FR',\n",
    "'Tire Slip Angle RL','Tire Slip Angle RR','Tire Slip Ratio FL','Tire Slip Ratio FR','Tire Slip Ratio RL','Tire Slip Ratio RR',\n",
    "'Tire Temp Core FL','Tire Temp Core FR','Tire Temp Core RL','Tire Temp Core RR','Tire Temp Inner FL','Tire Temp Inner FR',\n",
    "'Tire Temp Inner RL','Tire Temp Inner RR','Tire Temp Middle FL','Tire Temp Middle FR','Tire Temp Middle RL',\n",
    "'Tire Temp Middle RR','Tire Temp Outer FL','Tire Temp Outer FR','Tire Temp Outer RL','Tire Temp Outer RR','Toe In FL',\n",
    "'Toe In FR','Toe In RL','Toe In RR','Wheel Angular Speed FL','Wheel Angular Speed FR','Wheel Angular Speed RL','Wheel Angular Speed RR',\n",
    "'CG Distance','Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration','level']\n",
    "\n",
    "\n",
    "#Hyper-parameters\n",
    "num_epochs = 100\n",
    "batches = 1\n",
    "learning_rate = 0.01\n",
    "input_size = len(left_column)-1 # left column except 'level'\n",
    "output_size = 2 # Expert and Beginner\n",
    "hidden_size = 68 # ?\n",
    "num_layers = 1\n",
    "num_begin_train = round(num_begin*0.75)*(number_of_corner-start)\n",
    "num_exp_train = round(num_exp*0.75)*(number_of_corner-start)\n",
    "num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "\n",
    "print(num_begin_train, num_exp_train,num_begin_test,num_exp_test)\n",
    "aug = 1\n",
    "## Define GRU, Loss func and Optimizer\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "#         out, _ = self.gru(x, h0)\n",
    "        out, _ = self.lstm(x, (h0,c0)) \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "gru = GRU(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)  \n",
    "\n",
    "gru.fc.weight.data.fill_(1)\n",
    "gru.fc.bias.data.fill_(1)\n",
    "print(gru.fc.weight,gru.fc.bias)\n",
    "\n",
    "## Data Processing\n",
    "array_x = []\n",
    "array_y = []\n",
    "input_x = []\n",
    "input_y = []\n",
    "n_row = []\n",
    "\n",
    "df_tmp_begin = pd.DataFrame() \n",
    "df_tmp_exp = pd.DataFrame() \n",
    "for curve_num in range(start,number_of_corner):\n",
    "    df_tmp_begin = pd.concat([df_tmp_begin,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_begin.csv')])\n",
    "    df_tmp_exp   = pd.concat([df_tmp_exp,pd.read_csv('cornerData/corner_'+str(curve_num+1)+'_expert.csv')])    \n",
    "df_curve1 = pd.concat([df_tmp_begin, df_tmp_exp], ignore_index=True) \n",
    "df_curve1 = df_curve1.loc[:,left_column]\n",
    "df_curve1_saved = df_curve1.loc[:,left_column] # data backup\n",
    "df_curve1.to_csv('cornerData/corner_'+'_dfcurve1'+'.csv')\n",
    "\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x)\n",
    "# scaler.mean_\n",
    "# x = scaler.transform(x)\n",
    "# print(x_normal)\n",
    "# # print(x)\n",
    "# # print(x['Distance'])\n",
    "# print(type(x))\n",
    "# print(type(x_normal))\n",
    "\n",
    "datum = df_curve1_saved\n",
    "yyy = datum.pop('level')\n",
    "left = left_column.remove('level')\n",
    "for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "#     x = df_curve1_saved.loc[0:num_row[i]-1\n",
    "    y = yyy.loc[0:num_row[i]-1]\n",
    "    x_original = datum.loc[0:num_row[i]-1]\n",
    "#     print(x_original)\n",
    "#     print(num_row[i],y[0],x_original.iloc[-1,0])\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "#     scaler.fit(x_original)\n",
    "#     scaler.mean_\n",
    "    x_normal = scaler.fit_transform(x_original)\n",
    "    x_normal = scaler.transform(x_original)\n",
    "    \n",
    "    \n",
    "    x_normal = np.pad(x_normal,[(0,sequence_length-num_row[i]),(0,0)]) #post padding\n",
    "#     x_normal = np.pad(x_normal,[(sequence_length-num_row[i],0),(0,0)]) #pre padding\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame(x_normal,columns=left)\n",
    "#     print(datum)\n",
    "#     print(i)\n",
    "#     print(num_row)\n",
    "#     print(num_row[i])\n",
    "    datum.drop(range(0,num_row[i]),inplace=True)\n",
    "    datum.reset_index(drop=True, inplace=True)\n",
    "    yyy.drop(range(0,num_row[i]),inplace=True)\n",
    "    yyy.reset_index(drop=True, inplace=True)\n",
    "    # y = x.pop('level')\n",
    "    \n",
    "#     # DATA Augmentation\n",
    "#     nan = pd.DataFrame(np.nan,columns=range(x.shape[1]),index=range(x.shape[0]))\n",
    "#     alter = pd.concat([x,nan]).sort_index()\n",
    "#     alter = alter.interpolate()\n",
    "#     alter.reset_index(drop=True, inplace=True)\n",
    "#     x_aug = alter[alter.index%2==1]\n",
    "\n",
    "    \n",
    "    array_x.append(x)\n",
    "#     array_x.append(x_aug)\n",
    "    array_y.append(y)\n",
    "#     array_y.append(y)\n",
    "\n",
    "    \n",
    "## Randomize sequence \n",
    "sequence = np.arange((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug)\n",
    "np.random.seed(11)\n",
    "np.random.shuffle(sequence)\n",
    "\n",
    "# sequence = [0,1,2,15,4,5,6,7,8,9,18,11,12,13,14,19,20,21,34,23,24,25,26,27,28,37,30,31,32,33,3,16,17,10,22,35,36,29]\n",
    "print(sequence)\n",
    "\n",
    "# # Data Augmentation\n",
    "# num_row = pd.Series(num_row)\n",
    "# num_row = num_row.repeat(2)\n",
    "# # sequence = pd.Series(sequence)\n",
    "# # sequence = sequence.repeat(2)\n",
    "# num_row.reset_index(drop=True, inplace=True)\n",
    "# # sequence.reset_index(drop=True, inplace=True)\n",
    "# print(num_row, sequence)\n",
    "\n",
    "for i in sequence:\n",
    "    input_x.append(array_x[i])\n",
    "    input_y.append(array_y[i])\n",
    "#     n_row.append(num_row[i])\n",
    "    n_row.append(sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "test_list=[]\n",
    "count = 0\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "print((num_begin_train + num_exp_train)*aug)\n",
    "\n",
    "# start = time.time()  # 시작 시간 저장\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0,(num_begin_train + num_exp_train)*aug):\n",
    "        \n",
    "#         print(i)\n",
    "#         print(len(input_x))\n",
    "        # array type (numpy) 앞\n",
    "        X = np.array(input_x[i])\n",
    "#         print(X)\n",
    "        X = X.reshape(-1,n_row[i],input_size)\n",
    "#         print(X)\n",
    "        Y = np.array(input_y[i])   \n",
    "#         print(X,X.shape,Y[0])\n",
    "#         time.sleep(300)\n",
    "        # tensor type (pytorch)\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.float()\n",
    "        Y = torch.tensor([Y[0]])\n",
    "        Y = Y.type(torch.LongTensor)\n",
    "#         Y = Y.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = gru(X)\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         count += 1\n",
    "#         loss_list.append(loss.data)\n",
    "#         iteration_list.append(count)\n",
    "#         accuracy_list.append(accuracy)\n",
    "#         print (f'Loss: {loss.item():.4f}')\n",
    "    count += 1\n",
    "    loss_list.append(loss.data)\n",
    "    iteration_list.append(count)\n",
    "    print (f'Epoch: [{epoch}/{num_epochs}]' f'Loss: {loss.item():.4f}')\n",
    "## Test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for i in range((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug):\n",
    "        \n",
    "        # array type (numpy)\n",
    "        print(i)\n",
    "        X = np.array(input_x[i])\n",
    "        X = X.reshape(-1,n_row[i],input_size)\n",
    "        Y = np.array(input_y[i])   \n",
    "\n",
    "        # tensor type (pytorch)\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.float()\n",
    "        Y = torch.tensor([Y[0]])\n",
    "        Y = Y.type(torch.LongTensor)\n",
    "#         Y = Y.float()\n",
    "        output = gru(X)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_samples += Y.size(0)\n",
    "        n_correct += (predicted == Y).sum().item()\n",
    "        print(Y, predicted)\n",
    "        \n",
    "\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {(num_begin_test + num_exp_test)*aug} test images: {acc} %')\n",
    "#     print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "print(f'Accuracy of the network on the {num_begin_test + num_exp_test} test images: {acc} %')\n",
    "# print(predicted, Y, output)\n",
    "print(gru.fc.weight,gru.fc.bias)\n",
    "# print(iteration_list)\n",
    "# # visualization accuracy \n",
    "# plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
    "# plt.savefig('graph.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = 'bread'\n",
    "vars()['cat'] = 123\n",
    "print(cat)\n",
    "print(type(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame() \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([0, 1, np.nan, 10])\n",
    "s.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x)\n",
    "x.shape\n",
    "nan = pd.DataFrame(np.nan,columns=range(x.shape[1]),index=range(x.shape[0]))\n",
    "alter = pd.concat([x,nan]).sort_index()\n",
    "alter = alter.interpolate()\n",
    "alter.reset_index(drop=True, inplace=True)\n",
    "# print(alter[alter.index%2==0])\n",
    "# print(alter[alter.index%2==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# print(x['Distance'])\n",
    "# scaler = StandardScaler()\n",
    "# print(scaler.fit(x))\n",
    "# print(scaler.mean_)\n",
    "# # print(scaler.transform(x))\n",
    "# x_normal = scaler.transform(x)\n",
    "# print(x_normal)\n",
    "# # print(x)\n",
    "# # print(x['Distance'])\n",
    "# print(type(x))\n",
    "# print(type(x_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f\n",
    "# print(gru.fc.weight,gru.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "a = np.array([1,2,3])\n",
    "a = pd.DataFrame([1,2,3])\n",
    "print(a)\n",
    "# a.repeat(2)\n",
    "# np.array(num_row).repeat(2)\n",
    "# num_row = pd.Series(num_row)\n",
    "# num_row.repeat(2)\n",
    "print(num_row)\n",
    "type(num_row)\n",
    "abc = pd.Series(num_row)\n",
    "abc.repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_begin_train = round(num_begin*0.75)*(number_of_corner-start)\n",
    "# num_exp_train = round(num_exp*0.75)*(number_of_corner-start)\n",
    "# num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "# num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "# # print(num_begin_train, num_exp_train,num_begin_test,num_exp_test)\n",
    "# num_begin_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
