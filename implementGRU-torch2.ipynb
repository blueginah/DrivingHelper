{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d03d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57751f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "# from optuna.trial import TrialState\n",
    "import os\n",
    "import joblib\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0feeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-19 22:08:27,344]\u001b[0m A new study created in memory with name: no-name-8f0d9c61-539d-4bab-9e0e-6f3c6245ef22\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10] [19 20 21 22 23 24 25 26 27 28 29]\n",
      "[ 3  5  7  8  1  4 10  6  9  0  2 22 24 26 27 20 23 29 25 28 19 21 12 15\n",
      " 14 16 17 18 11 13 31 34 33 35 36 37 30 32]\n",
      "Epoch: [0/100]Loss: 0.8999\n",
      "Accuracy of the network on the 16 test images: 58.333333333333336 %\n",
      "Epoch: [1/100]Loss: 0.6998\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [2/100]Loss: 0.6106\n",
      "Accuracy of the network on the 16 test images: 83.33333333333333 %\n",
      "Epoch: [3/100]Loss: 0.5772\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [4/100]Loss: 0.5216\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [5/100]Loss: 0.4270\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [6/100]Loss: 0.2945\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [7/100]Loss: 0.1420\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [8/100]Loss: 0.0528\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [9/100]Loss: 0.0113\n",
      "Accuracy of the network on the 16 test images: 75.0 %\n",
      "Epoch: [10/100]Loss: 0.0053\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [11/100]Loss: 0.0046\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [12/100]Loss: 0.0015\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [13/100]Loss: 0.0005\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [14/100]Loss: 0.0004\n",
      "Accuracy of the network on the 16 test images: 91.66666666666667 %\n",
      "Epoch: [15/100]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [16/100]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [17/100]Loss: 0.0003\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [18/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [19/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [20/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [21/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [22/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [23/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [24/100]Loss: 0.0002\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [25/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [26/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [27/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [28/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [29/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [30/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [31/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [32/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [33/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [34/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [35/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [36/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [37/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [38/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [39/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [40/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [41/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [42/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [43/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [44/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [45/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [46/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [47/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [48/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [49/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [50/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [51/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [52/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [53/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [54/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [55/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [56/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [57/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [58/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [59/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [60/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [61/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [62/100]Loss: 0.0001\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [63/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [64/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [65/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [66/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [67/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [68/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [69/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [70/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [71/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [72/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [73/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [74/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [75/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [76/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [77/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [78/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [79/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [80/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [81/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [82/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [83/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [84/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [85/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [86/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [87/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [88/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [89/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [90/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [91/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [92/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [93/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [94/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [96/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [97/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [98/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Epoch: [99/100]Loss: 0.0000\n",
      "Accuracy of the network on the 16 test images: 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-19 22:09:34,930]\u001b[0m Trial 0 finished with value: 100.0 and parameters: {}. Best is trial 0 with value: 100.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 16 test images: 100.0 %\n",
      "Loss: 0.0000\n",
      "   number  value               duration\n",
      "0       0  100.0 0 days 00:01:07.585592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_list = [0,1,2,3,4,5,0]\n",
    "number_of_corner_list = [1,2,3,4,5,6,6]\n",
    "feature_list = [0, 1]\n",
    "truncate_number = 60\n",
    "for i in range(1):\n",
    "    feature = feature_list[i]\n",
    "    for i in range(1):\n",
    "        start = start_list[i]\n",
    "        number_of_corner = number_of_corner_list[i]\n",
    "        df_begin = []\n",
    "        num_row = []\n",
    "        num_row_corner = []\n",
    "        percent = []\n",
    "    #     start = 0s\n",
    "    #     number_of_corner = 1\n",
    "        f_1 = 'beginner_expert_processedData/beginner/beginner_'\n",
    "        f_3 = '.csv'\n",
    "        num_begin = 19\n",
    "#         curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1212.3, 1437.1]]\n",
    "        curveList = [[103.9, 209.3], [316.6, 399.6], [425.3, 517.9], [590.5, 756.9], [1048.7, 1110.5], [1212.3, 1437.1]]\n",
    "#         curveList = [[103.9, 156.6], [156.6, 209.3], [316.6, 358.1], [358.1, 399.6], \n",
    "#                      [425.3, 471.6], [471.6, 517.9],p[590.5, 673.7], [673.7, 756.9],\n",
    "#                      [1048.7, 1079.6], [1079.6, 1110.5], [1212.3, 1324.7], [1324.7, 1437.1]]\n",
    "\n",
    "        df_concat = pd.DataFrame()\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            for idx in range(1, num_begin+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =0\n",
    "                tmp['curve_number'] = curve_num\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "                num_row.append(np.size(tmpcorner,0)) \n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "                df_begin.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_begin[idx-1]])      \n",
    "            df_concat.to_csv('cornerData2/corner_'+str(curve_num+1)+'_begin'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_begin = []\n",
    "        df_exp = []\n",
    "        f_1 = 'beginner_expert_processedData/expert/expert_'\n",
    "        f_3 = '.csv'\n",
    "        num_exp = 19\n",
    "        \n",
    "        \n",
    "        df_concat = pd.DataFrame()\n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            for idx in range(1, num_exp+1):\n",
    "                tmp_file = f_1+str(idx)+'_new2'+f_3\n",
    "                df = pd.read_csv(tmp_file)\n",
    "                df = df.dropna()\n",
    "                tmp = df.astype(float)\n",
    "                tmp['level'] =1\n",
    "                tmp['curve_number'] = curve_num\n",
    "                tmpcorner = tmp[(tmp['Distance'] >= curveList[curve_num][0]) & (tmp['Distance'] <= curveList[curve_num][1])]\n",
    "                num_row.append(np.size(tmpcorner,0))\n",
    "                num_row_corner.append(np.size(tmpcorner,0)) \n",
    "                df_exp.append(tmpcorner)\n",
    "                df_concat = pd.concat([df_concat,df_exp[idx-1]])\n",
    "            df_concat.to_csv('cornerData2/corner_'+str(curve_num+1)+'_expert'+'.csv')\n",
    "            df_concat = pd.DataFrame()\n",
    "            df_exp = []\n",
    "            num_row_corner = np.array(num_row_corner)\n",
    "            per = 60  ##truncate to 60\n",
    "            percent.append(per)\n",
    "            num_row_corner = []\n",
    "        sequence_length = min(num_row)\n",
    "        if sequence_length < truncate_number:\n",
    "            sequnce_length = truncate_number + 5\n",
    "        mean_row = round(np.mean(num_row))\n",
    "        mean_row = min(num_row)\n",
    "\n",
    "        if feature == 0:\n",
    "            left_column = [\n",
    "                'curve_number',\n",
    "                'GPS Latitude','GPS Longitude',\n",
    "                'CG Distance',\n",
    "                'Damper Velocity (Calc) FL','Damper Velocity (Calc) FR','Damper Velocity (Calc) RL',\n",
    "            'Damper Velocity (Calc) RR','Corr Speed','Brake Pos',\n",
    "            'CG Accel Lateral','CG Accel Longitudinal','CG Accel Vertical','CG Height','Camber FL','Camber FR','Camber RL','Camber RR','Car Coord X',\n",
    "            'Car Coord Y','Car Coord Z','Car Pos Norm','Chassis Pitch Angle','Chassis Pitch Rate','Chassis Roll Angle','Chassis Roll Rate',\n",
    "            'Chassis Velocity X','Chassis Velocity Y','Chassis Velocity Z','Chassis Yaw Rate','Drive Train Speed','Engine RPM','Ground Speed',\n",
    "            'Ride Height FL','Ride Height FR','Ride Height RL','Ride Height RR','Road Temp','Self Align Torque FL','Self Align Torque FR',\n",
    "            'Self Align Torque RL','Self Align Torque RR','Steering Angle','Suspension Travel FL','Suspension Travel FR',\n",
    "            'Suspension Travel RL','Suspension Travel RR','Tire Load FL','Tire Load FR','Tire Load RL','Tire Load RR','Tire Loaded Radius FL',\n",
    "            'Tire Loaded Radius FR','Tire Loaded Radius RL','Tire Loaded Radius RR','Tire Pressure FL','Tire Pressure FR','Tire Pressure RL','Tire Pressure RR',\n",
    "            'Tire Rubber Grip FL','Tire Rubber Grip FR','Tire Rubber Grip RL','Tire Rubber Grip RR','Tire Slip Angle FL','Tire Slip Angle FR',\n",
    "            'Tire Slip Angle RL','Tire Slip Angle RR','Tire Slip Ratio FL','Tire Slip Ratio FR','Tire Slip Ratio RL','Tire Slip Ratio RR',\n",
    "            'Tire Temp Core FL','Tire Temp Core FR','Tire Temp Core RL','Tire Temp Core RR','Tire Temp Inner FL','Tire Temp Inner FR',\n",
    "            'Tire Temp Inner RL','Tire Temp Inner RR','Tire Temp Middle FL','Tire Temp Middle FR','Tire Temp Middle RL',\n",
    "            'Tire Temp Middle RR','Tire Temp Outer FL','Tire Temp Outer FR','Tire Temp Outer RL','Tire Temp Outer RR','Toe In FL',\n",
    "            'Toe In FR','Toe In RL','Toe In RR','Wheel Angular Speed FL','Wheel Angular Speed FR','Wheel Angular Speed RL','Wheel Angular Speed RR',\n",
    "            'Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration','level']\n",
    "\n",
    "        else:\n",
    "            left_column = ['curve_number','Brake Pos', 'Ground Speed', 'Steering Angle', 'Throttle Pos', 'Chassis Yaw Rate', 'Chassis Velocity X',\n",
    "                               'Chassis Velocity Y','Chassis Velocity Z','Lateral Velocity','Longitudinal Velocity','Lateral Acceleration','Longitudinal Acceleration',\n",
    "                               'CG Distance',\n",
    "                           'level']\n",
    "\n",
    "        #Hyper-parameters\n",
    "        num_epochs = 50\n",
    "        batches = 1\n",
    "        learning_rate = 0.001\n",
    "        input_size = len(left_column)-1 # left column except 'level'\n",
    "        output_size = 2 # Expert and Beginner\n",
    "        hidden_size = 100 # ?\n",
    "        num_layers = 2\n",
    "        num_begin_train = round(num_begin*0.60)*(number_of_corner-start)\n",
    "        num_exp_train = round(num_exp*0.60)*(number_of_corner-start)\n",
    "        num_begin_test = num_begin*(number_of_corner-start) - num_begin_train\n",
    "        num_exp_test = num_exp*(number_of_corner-start) - num_exp_train\n",
    "        aug = 1\n",
    "\n",
    "        \n",
    "        class GRU(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "                super(GRU, self).__init__()\n",
    "                self.num_layers = num_layers\n",
    "                self.hidden_size = hidden_size\n",
    "                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "            def forward(self, x):\n",
    "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "                out, _ = self.gru(x, h0)\n",
    "                out = out[:, -1, :]\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "        \n",
    "\n",
    "        ## Data Processing\n",
    "        array_x = []\n",
    "        array_y = []\n",
    "        input_x = []\n",
    "        input_y = []\n",
    "        n_row = []\n",
    "\n",
    "        df_tmp_begin = pd.DataFrame() \n",
    "        df_tmp_exp = pd.DataFrame() \n",
    "        for curve_num in range(start,number_of_corner):\n",
    "            df_tmp_begin = pd.concat([df_tmp_begin,pd.read_csv('cornerData2/corner_'+str(curve_num+1)+'_begin.csv')])\n",
    "            df_tmp_exp   = pd.concat([df_tmp_exp,pd.read_csv('cornerData2/corner_'+str(curve_num+1)+'_expert.csv')])    \n",
    "        df_curve1 = pd.concat([df_tmp_begin, df_tmp_exp], ignore_index=True) \n",
    "        df_curve1 = df_curve1.loc[:,left_column]\n",
    "        df_curve1_saved = df_curve1.loc[:,left_column] # data backup\n",
    "        df_curve1.to_csv('cornerData2/corner_'+'_dfcurve1'+'.csv')\n",
    "        datum = df_curve1_saved\n",
    "        yyy = datum.pop('level')\n",
    "        left = left_column.remove('level')\n",
    "        for i in range(0,num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start)):\n",
    "            y = yyy.loc[0:num_row[i]-1]\n",
    "            x_original = datum.loc[0:num_row[i]-1]\n",
    "            scaler = MinMaxScaler()\n",
    "            x_normal = scaler.fit_transform(x_original)\n",
    "            x_normal = scaler.transform(x_original)\n",
    "            x_normal = np.pad(x_normal,[(0,60),(0,0)],'edge') #post padding ## edge padding\n",
    "            x = pd.DataFrame(x_normal,columns=left)\n",
    "            p = i//(num_begin*2)\n",
    "            x = x.truncate(after=percent[p]-1)\n",
    "            datum.drop(range(0,num_row[i]),inplace=True)\n",
    "            datum.reset_index(drop=True, inplace=True)\n",
    "            yyy.drop(range(0,num_row[i]),inplace=True)\n",
    "            yyy.reset_index(drop=True, inplace=True)\n",
    "            array_x.append(x)\n",
    "            array_y.append(y)\n",
    "\n",
    "\n",
    "\n",
    "        ## Randomize sequence \n",
    "        # sequence = np.arange((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug/2)\n",
    "        seq_train_begin = np.arange(num_begin_train)\n",
    "        seq_test_begin = np.arange(num_begin_test) + num_begin_train\n",
    "        seq_train_exp = seq_train_begin + num_begin*(number_of_corner-start)\n",
    "        seq_test_exp = seq_test_begin + num_begin*(number_of_corner-start)\n",
    "        print(seq_train_begin,seq_train_exp)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_train_begin)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_test_begin)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_train_exp)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(seq_test_exp)\n",
    "\n",
    "\n",
    "        sequence = np.concatenate((seq_train_begin, seq_train_exp, seq_test_begin, seq_test_exp), axis=None)\n",
    "        sequence = sequence.astype('int')\n",
    "        print(sequence)\n",
    "\n",
    "\n",
    "        for i in sequence:\n",
    "            input_x.append(array_x[i])\n",
    "            input_y.append(array_y[i])\n",
    "            p = i//(num_begin*2)\n",
    "            n_row = n_row + [percent[p]]\n",
    "        \n",
    "    \n",
    "            \n",
    "        def train_mnist(trial):\n",
    "            \n",
    "            d2 = {'test sample': [], 'loss': [], 'accuracy': [], 'epoch' : [], 'batch':[],'learning rate' :[], 'hidden size':[],'hidden_layer':[]}\n",
    "            df2 = pd.DataFrame(data=d2)\n",
    "            loss_list = []\n",
    "            iteration_list = []\n",
    "            accuracy_list = []\n",
    "            test_list=[]\n",
    "            accuracy2_list=[]\n",
    "            count = 0\n",
    "            cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "              'train_batch_size' : 64,\n",
    "              'test_batch_size' : 1000,\n",
    "              'n_epochs' : 1,\n",
    "              'seed' : 0,\n",
    "              'log_interval' : 100,\n",
    "              'save_model' : False,\n",
    "              'lr'       : 0.0008674236780492814,          \n",
    "              'hidden_size' : 126,\n",
    "#               'lr'       : trial.suggest_loguniform('lr', 5e-4, 5e-3),          \n",
    "#               'hidden_size' : trial.suggest_int('hidden_size', 60, 200),\n",
    "              'optimizer': optim.Adam,\n",
    "              'activation': F.relu}\n",
    "        \n",
    "            torch.manual_seed(cfg['seed'])\n",
    "        \n",
    "            gru = GRU(input_size, cfg['hidden_size'], num_layers, output_size)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)       \n",
    "            optimizer = cfg['optimizer'](gru.parameters(), lr=cfg['lr'])\n",
    "    \n",
    "    \n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0,(num_begin_train + num_exp_train)*aug):\n",
    "                    # array type (numpy) 앞\n",
    "                    X = np.array(input_x[i])\n",
    "                    X = X.reshape(-1,n_row[i],input_size)\n",
    "                    Y = np.array(input_y[i])\n",
    "\n",
    "                    # tensor type (pytorch)\n",
    "                    X = torch.from_numpy(X)\n",
    "                    X = X.float()\n",
    "                    Y = torch.tensor([Y[0]])\n",
    "                    Y = Y.type(torch.LongTensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = gru(X)\n",
    "                    loss = criterion(output, Y)\n",
    "\n",
    "                    # Backward and optimize\n",
    "            #         optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                count += 1\n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                print (f'Epoch: [{epoch}/{num_epochs}]' f'Loss: {loss.item():.4f}')\n",
    "            ## Test\n",
    "                with torch.no_grad():\n",
    "                    n_correct = 0\n",
    "                    n_correct2 = 0\n",
    "                    n_samples = 0\n",
    "                    n_samples2 = 0\n",
    "\n",
    "                    for i in range((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug-4):\n",
    "\n",
    "                        # array type (numpy)\n",
    "                        X = np.array(input_x[i])\n",
    "                        X = X.reshape(-1,n_row[i],input_size)\n",
    "                        Y = np.array(input_y[i])\n",
    "\n",
    "                        # tensor type (pytorch)\n",
    "                        X = torch.from_numpy(X)\n",
    "                        X = X.float()\n",
    "                        Y = torch.tensor([Y[0]])\n",
    "                        Y = Y.type(torch.LongTensor)\n",
    "                        output = gru(X)\n",
    "                        _, predicted = torch.max(output.data, 1)\n",
    "                        n_samples += Y.size(0)\n",
    "                        n_correct += (predicted == Y).sum().item()\n",
    "\n",
    "    \n",
    "                    for i in range((num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug-4,(num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug):\n",
    "\n",
    "                        # array type (numpy)\n",
    "                        X = np.array(input_x[i])\n",
    "                        X = X.reshape(-1,n_row[i],input_size)\n",
    "                        Y = np.array(input_y[i])\n",
    "\n",
    "                        # tensor type (pytorch)\n",
    "                        X = torch.from_numpy(X)\n",
    "                        X = X.float()\n",
    "                        Y = torch.tensor([Y[0]])\n",
    "                        Y = Y.type(torch.LongTensor)\n",
    "                        output = gru(X)\n",
    "                        _, predicted2 = torch.max(output.data, 1)\n",
    "                        n_samples2 += Y.size(0)\n",
    "                        n_correct2 += (predicted2 == Y).sum().item()\n",
    "    \n",
    "    \n",
    "                    acc = 100.0 * n_correct / n_samples\n",
    "                    acc2 = 100.0 * n_correct2 / n_samples2\n",
    "                    accuracy_list.append(acc)\n",
    "                    accuracy2_list.append(acc2)\n",
    "                    print(f'Accuracy of the network on the {(num_begin_test + num_exp_test)*aug} test images: {acc} %')          \n",
    "            plt.plot(iteration_list,loss_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"GRU: Loss vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1{curve_num}.png')\n",
    "                plt.clf()\n",
    "\n",
    "            plt.plot(iteration_list,accuracy_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\" Validation Accuracy\")\n",
    "            plt.title(\"GRU: Validaiton Accuracy vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1_acc{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1_acc{curve_num}.png')\n",
    "                plt.clf()    \n",
    "\n",
    "            plt.plot(iteration_list,accuracy2_list)\n",
    "            plt.xlabel(\"Number of iteration\")\n",
    "            plt.ylabel(\" Test Accuracy\")\n",
    "            plt.title(\"GRU: Test Accuracy vs Number of iteration\")\n",
    "\n",
    "            if feature == 0:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_allFeat1_acc{curve_num}.png')\n",
    "                plt.clf()\n",
    "            else:\n",
    "                plt.savefig(f'cornerData2/loss_VS_epoch_corner{number_of_corner}_{number_of_corner-start}_selectedFeat1_acc{curve_num}.png')\n",
    "                plt.clf()  \n",
    "\n",
    "            print(f'Accuracy of the network on the {num_begin_test + num_exp_test} test images: {acc} %')\n",
    "            print(f'Loss: {loss.item():.4f}')\n",
    "            return acc\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(train_mnist, n_trials=1)\n",
    "        joblib.dump(study, f'cornerData2/mnist_optuna{curve_num}.pkl')\n",
    "    \n",
    "        study = joblib.load(f'cornerData2/mnist_optuna{curve_num}.pkl')\n",
    "        df3 = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "        print(df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e9dae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # df2.to_csv('cornerData/result_gru.csv')\n",
    "# # d2 = {'test sample': [], 'loss': [], 'accuracy': [], 'epoch' : [], 'batch':[],'learning rate' :[], 'hidden size':[],'hidden_layer':[]}\n",
    "# # df2 = pd.DataFrame(data=d2)\n",
    "# d = {'test sample': [num_begin_test + num_exp_test], 'loss': [loss.item()], 'accuracy': [acc], 'epoch' : [num_epochs], 'batch':[batches],'learning rate' :[learning_rate], 'hidden size':[hidden_size],'hidden_layer':[num_layers]}\n",
    "# df = pd.DataFrame(data=d)\n",
    "# df2 = df2.append(df+4)\n",
    "# print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9f60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21\n",
      "[[0.         1.         1.         ... 0.01574803 0.91111111 0.65517241]\n",
      " [0.         0.99451124 0.98900871 ... 0.09448819 0.86666667 0.51724138]\n",
      " [0.         0.98849974 0.97787916 ... 0.03149606 0.77777778 0.34482759]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.92125984 0.33333333 0.34482759]\n",
      " [0.         0.         0.         ... 0.92125984 0.33333333 0.34482759]\n",
      " [0.         0.         0.         ... 0.92125984 0.33333333 0.34482759]]\n",
      "[111, 98, 105, 88, 115, 105, 112, 95, 94, 108, 127, 140, 135, 99, 97, 86, 90, 111, 97, 87, 86, 86, 84, 81, 81, 111, 88, 80, 78, 77, 77, 84, 80, 77, 79, 79, 80, 84]\n"
     ]
    }
   ],
   "source": [
    "# x_normal = np.pad(x_normal,[(0,sequence_length-num_row[i]),(0,0)],'edge')\n",
    "# print(x_normal)\n",
    "# percent.append(60)\n",
    "# print(percent)\n",
    "print(sequence_length-num_row[1])\n",
    "print(x_normal)\n",
    "print(num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ffd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2305893d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cornerData2/mnist_optuna0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-da5336ab8e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcurve_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'cornerData2/mnist_optuna{curve_num}.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'datetime_start'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'datetime_complete'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ssdkms\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cornerData2/mnist_optuna0.pkl'"
     ]
    }
   ],
   "source": [
    "for curve_num in range(6):\n",
    "    study = joblib.load(f'cornerData2/mnist_optuna{curve_num}.pkl')\n",
    "    df3 = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "    print(df3)\n",
    "    df3=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((num_begin_train + num_exp_train)*aug, (num_begin*(number_of_corner-start) + num_exp*(number_of_corner-start))*aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(input_size, cfg['hidden_size'], num_layers, output_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
